{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71dd278",
   "metadata": {},
   "source": [
    "Sensor Stream  →  Per-Engine Buffer  →  LSTM  →  RUL Prediction\n",
    "                     (last 30 cycles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf86373",
   "metadata": {},
   "source": [
    "{\n",
    "  \"buffer\": deque(maxlen=WINDOW),\n",
    "  \"last_prediction\": float\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b138304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91903\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "C:\\Users\\91903\\AppData\\Local\\Temp\\ipykernel_23904\\3007994451.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# --------------------\n",
    "# 1. Load artifacts\n",
    "# --------------------\n",
    "MODEL_PATH = \"rul_lstm_fd001.pth\"\n",
    "SCALER_PATH = \"scaler_fd001.pkl\"\n",
    "FEATURES_PATH = \"features_fd001.pkl\"\n",
    "ARCH_PATH = \"model_arch_fd001.pkl\"\n",
    "\n",
    "with open(SCALER_PATH, \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "with open(FEATURES_PATH, \"rb\") as f:\n",
    "    feature_cols = pickle.load(f)\n",
    "\n",
    "with open(ARCH_PATH, \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "# --------------------\n",
    "# 2. Recreate model\n",
    "# --------------------\n",
    "class RULLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n[-1])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = RULLSTM(input_dim=meta[\"input_dim\"], hidden_dim=meta[\"hidden_dim\"]).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "window = meta[\"window\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class RULStreamPredictor:\n",
    "    def __init__(self, model, scaler, feature_cols, window, device):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window = window\n",
    "        self.device = device\n",
    "        self.buffers = {}\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def process_cycle(self, engine_id, raw_features):\n",
    "        \"\"\"\n",
    "        raw_features: dict {feature_name: value}\n",
    "        returns: RUL prediction or None\n",
    "        \"\"\"\n",
    "\n",
    "        # init buffer\n",
    "        if engine_id not in self.buffers:\n",
    "            self.buffers[engine_id] = deque(maxlen=self.window)\n",
    "\n",
    "        # feature vector in correct order\n",
    "        x = np.array([raw_features[c] for c in self.feature_cols]).reshape(1, -1)\n",
    "\n",
    "        # normalize using TRAIN scaler\n",
    "        x = self.scaler.transform(x)[0]\n",
    "\n",
    "        # append to buffer\n",
    "        self.buffers[engine_id].append(x)\n",
    "\n",
    "        # not enough history yet\n",
    "        if len(self.buffers[engine_id]) < self.window:\n",
    "            return None\n",
    "\n",
    "        # prepare tensor\n",
    "        window_data = np.array(self.buffers[engine_id]).reshape(1, self.window, -1)\n",
    "        xt = torch.tensor(window_data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rul_pred = self.model(xt).cpu().numpy().item()\n",
    "\n",
    "        return max(rul_pred, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a697e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = RULStreamPredictor(\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    feature_cols=feature_cols,\n",
    "    window=window,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "rul_predictions = {}\n",
    "\n",
    "for engine_id, engine_df in test_df.groupby(\"engine_id\"):\n",
    "    engine_df = engine_df.sort_values(\"cycle\")\n",
    "\n",
    "    for _, row in engine_df.iterrows():\n",
    "        raw_features = row[FEATURE_COLS].to_dict()\n",
    "\n",
    "        pred = streamer.process_cycle(engine_id, raw_features)\n",
    "\n",
    "        if pred is not None:\n",
    "            rul_predictions.setdefault(engine_id, []).append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = np.array([\n",
    "    rul_predictions[eid][-1] for eid in sorted(rul_predictions.keys())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eid = list(rul_predictions.keys())[0]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(rul_predictions[eid], label=\"Predicted RUL\")\n",
    "plt.xlabel(\"Cycle\")\n",
    "plt.ylabel(\"RUL\")\n",
    "plt.title(f\"Engine {eid} – Streaming RUL Prediction\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALERT_THRESHOLD = 20\n",
    "\n",
    "def maintenance_alert(rul):\n",
    "    if rul < ALERT_THRESHOLD:\n",
    "        return \"⚠️ MAINTENANCE REQUIRED\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert = maintenance_alert(pred)\n",
    "if alert:\n",
    "    print(f\"Engine {engine_id}: {alert} (RUL={pred:.1f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
